{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annealed importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper: https://arxiv.org/abs/physics/9803008\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIS implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define some distributions that we want to use for $f_n$ and $f_0$. In this example, I will use cauchy distribution for $f_n$ and Gaussian for $f_0$. Note that we do not need to know the exact PDF for both $f_n$ and $f_0$. For example, I define the $\\textrm{gaussian_num}$ as just \n",
    "\n",
    "$$e^{-0.5(\\frac{x - \\mu}{\\sigma})^2}$$\n",
    "\n",
    "because Gaussian PDF is proportional to this quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cauchy(x, x_0 = 0, gamma = 0.5):\n",
    "    return (1 / (np.pi * gamma)) * ((gamma ** 2) / ((x - x_0)**2 + gamma**2))\n",
    "\n",
    "def gaussian_num(x, mu = 0, sigma = 1):\n",
    "    return np.exp(-0.5 * ((x - mu) / sigma)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_n(x):\n",
    "    return cauchy(x)\n",
    "    \n",
    "def f_0(x):\n",
    "    return gaussian_num(x, mu = 10, sigma = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to define the sequence transition distributions. Usually, we take the interpolation between $f_n$ and $f_0$ as follows (see Eq. 3 in the paper):\n",
    "\n",
    "$$f_j(x) = f_0(x)^{\\beta_j} f_n(x)^{1 - \\beta_j},$$\n",
    "\n",
    "where $1 = \\beta_0 > \\beta_1 > \\dots > \\beta_n = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_j(x, beta):\n",
    "    return f_0(x)**beta * f_n(x)**(1 - beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define the transition operator. Some options include Metropolis/Metropolis-Hastings, Gibbs sampling, Hamiltonian Monte Carlo, etc. Here, we will use Metropolis sampling by assuming the proposal $q(x'|x) = \\mathcal{N}(x,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_sampling(x, f, steps = 10):\n",
    "    for _ in range(steps):\n",
    "        x_prime = x + np.random.normal() # sample x' from proposal q(x'|x) = N(x,I)\n",
    "        a = f(x_prime) / f(x) # compute acceptance probability for Metropolis sampling\n",
    "        if np.random.uniform() < a: # determine if x' is accepted\n",
    "            x = x_prime\n",
    "    return x\n",
    "\n",
    "\n",
    "def gibbs_sampling(x):\n",
    "    pass\n",
    "\n",
    "\n",
    "def hamiltonian_mc(x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When computing $w$, it is better to do it in $\\log$ space for numerical stability. Normally we compute the weight $w^{(i)}$ for the $i$-th sample $x^{(i)}$ as:\n",
    "\n",
    "$$w^{(i)} = \\frac{f_{n-1}(x_{n-1})}{f_{n}(x_{n-1})} \\frac{f_{n-2}(x_{n-2})}{f_{n-1}(x_{n-2})} \\dots \\frac{f_{0}(x_{0})}{f_{1}(x_{0})}$$\n",
    "\n",
    "In $\\log$ space, we have:\n",
    "\n",
    "$$\\log(w^{(i)}) = \\Big[\\log(f_{n-1}(x_{n-1})) - \\log(f_{n}(x_{n-1}))\\Big] + \\Big[\\log(f_{n-2}(x_{n-2})) - \\log(f_{n-1}(x_{n-2}))\\Big] + \\dots + \\Big[\\log(f_{0}(x_{0})) - \\log(f_{1}(x_{0}))\\Big]$$\n",
    "\n",
    "And since we are in the $\\log$ space, we start at $\\log(w) = 0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = 100 # number of samples to generate\n",
    "num_dist = 200  # number of passing/intermediate distributions\n",
    "betas = np.linspace(0, 1, num_dist)\n",
    "\n",
    "# Initialize a list to store the samples and their weights\n",
    "xs = []\n",
    "ws = []\n",
    "\n",
    "# AIS\n",
    "for _ in range(num_sample):\n",
    "    x = stats.cauchy.rvs(loc=0, scale=0.5) # sample initial point from proposal p_n\n",
    "    log_w = 0 # set initial log(w) to 0\n",
    "    \n",
    "    # Move from f_n -> f_{n-1} -> ... -> f_0\n",
    "    for i in range(1, num_dist):\n",
    "        f = lambda x: f_j(x, betas[i])\n",
    "        x = metropolis_sampling(x, f, steps = 20) # sample x' from f_j with Metropolis sampling\n",
    "        log_w += np.log(f_j(x, betas[i])) - np.log(f_j(x, betas[i-1])) # compute weight\n",
    "    \n",
    "    # Store the sample and its weight\n",
    "    xs.append(x)\n",
    "    ws.append(np.exp(log_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the expectation (see Eq. 12 in the paper):\n",
    "\n",
    "$$\\bar{a} = \\frac{\\sum_{i=1}^N w^{(i)} a(x^{(i)})}{\\sum_{i=1}^N w^{(i)}}$$\n",
    "\n",
    "Which in this case should give us a value close to the mean of the target Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.99670948476493\n"
     ]
    }
   ],
   "source": [
    "# Compute expectation of the target distribution\n",
    "a_bar = np.dot(ws, xs) / np.sum(ws)\n",
    "\n",
    "print(a_bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also be able to compute $p_0(x = x_{test})$, since $p_0(x) = \\frac{f_0}{Z_0}$, and $\\frac{Z_0}{Z_n} = \\frac{\\sum_{i=1}^N w^{(i)}}{N}$. Since we know the full form of $p_n(x)$, then $Z_n = 1$, so $Z_0 = \\frac{\\sum_{i=1}^N w^{(i)}}{N}$. To check if the estimated $Z_0$ is close to the actual value, we will compare it with $\\sigma \\sqrt{2\\pi}$ since it is the actual normalization constant for $\\mathcal{N}(\\mu, \\sigma)$. And to check if the estimated $p_0(x = x_{test})$ is close to the actual value, we will evaluate $p_0(x = x_{test})$ using $\\textrm{stats.norm.pdf(x_test)}$ from scipy.stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated values\n",
      "================\n",
      "Z_0:  5.654357586938124\n",
      "p(x = x_test):  0.0001883089670748076 \n",
      "log p(x = x_test):  -8.577426502219899\n",
      "\n",
      "Actual values\n",
      "================\n",
      "Actual Z_0:  5.0132565492620005\n",
      "Actual p(x = x_test):  0.00021239013527537572 \n",
      "Actual log p(x = x_test):  -8.457085713764618\n"
     ]
    }
   ],
   "source": [
    "x_test = 2.6\n",
    "Z_0 = np.mean(ws)\n",
    "p_x_test = f_0(x_test) / Z_0\n",
    "\n",
    "print('Estimated values\\n================')\n",
    "print('Z_0: ', Z_0)\n",
    "print('p(x = x_test): ', p_x_test, \n",
    "      '\\nlog p(x = x_test): ', np.log(p_x_test)\n",
    "     )\n",
    "\n",
    "print('\\nActual values\\n================')\n",
    "print('Actual Z_0: ', 2 * ((2 * np.pi)**0.5))\n",
    "print('Actual p(x = x_test): ', stats.norm.pdf(x_test, 10, 2), \n",
    "      '\\nActual log p(x = x_test): ', np.log(stats.norm.pdf(x_test, 10, 2))\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
